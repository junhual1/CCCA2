{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parse tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "__________ Iteration 94 __________\n",
      "start searching\n",
      "Chunk file saved\n",
      "File readed\n",
      "End searching\n",
      "file saved:  ../data/filt_chunk/filtered_94.json\n",
      "file deleted:  ../data/filt_chunk/filtered_94.json\n",
      "\n",
      "\n",
      "__________ Iteration 95 __________\n",
      "start searching\n",
      "Chunk file saved\n",
      "File readed\n",
      "End searching\n",
      "file saved:  ../data/filt_chunk/filtered_95.json\n",
      "file deleted:  ../data/filt_chunk/filtered_95.json\n",
      "\n",
      "\n",
      "__________ Iteration 96 __________\n",
      "start searching\n",
      "Chunk file saved\n",
      "File readed\n",
      "End searching\n",
      "file saved:  ../data/filt_chunk/filtered_96.json\n",
      "file deleted:  ../data/filt_chunk/filtered_96.json\n",
      "\n",
      "\n",
      "__________ Iteration 97 __________\n",
      "start searching\n",
      "Chunk file saved\n",
      "File readed\n",
      "End searching\n",
      "file saved:  ../data/filt_chunk/filtered_97.json\n",
      "file deleted:  ../data/filt_chunk/filtered_97.json\n",
      "\n",
      "\n",
      "__________ Iteration 98 __________\n",
      "start searching\n",
      "Chunk file saved\n",
      "File readed\n",
      "End searching\n",
      "file saved:  ../data/filt_chunk/filtered_98.json\n",
      "file deleted:  ../data/filt_chunk/filtered_98.json\n",
      "\n",
      "\n",
      "__________ Iteration 99 __________\n",
      "start searching\n",
      "Chunk file saved\n",
      "File readed\n",
      "End searching\n",
      "file saved:  ../data/filt_chunk/filtered_99.json\n",
      "file deleted:  ../data/filt_chunk/filtered_99.json\n",
      "\n",
      "\n",
      "__________ Iteration 100 __________\n",
      "start searching\n",
      "Chunk file saved\n",
      "File readed\n",
      "End searching\n",
      "file saved:  ../data/filt_chunk/filtered_100.json\n",
      "file deleted:  ../data/filt_chunk/filtered_100.json\n",
      "\n",
      "\n",
      "__________ Iteration 101 __________\n",
      "start searching\n"
     ]
    }
   ],
   "source": [
    "import pyproj\n",
    "from shapely.geometry import Polygon\n",
    "from collections import defaultdict\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import pandasql as ps\n",
    "import warnings\n",
    "\n",
    "STATE_CODE = {1: \"new south wales\", 2: \"victoria\", 3: \"queensland\",\n",
    "            4: \"south australia\", 5: \"western australia\", 6: \"tasmania\",\n",
    "            7: \"northern territory\", 8: \"australian capital territory\", \n",
    "            9: \"offshore territories\"}\n",
    "DROP_PROTO = (\"new south wales, australia\", \"victoria, australia\", \"queensland, australia\",\n",
    "            \"south australia, australia\", \"western australia, australia\", \"tasmania, australia\",\n",
    "            \"northern territory, australia\", \"australian capital territory, australia\", \n",
    "            \"offshore territories, australia\", \"australia\")\n",
    "OUT_DIR = \"../data/filt_chunk/\"   ### filtered json file location\n",
    "CHUNK_PATH = \"../data/raw_chunk/\"   ### path to store extracted chunks\n",
    "TWEET_PATH = r\"/mnt/f/Downloads/twitter-huge.json\" ### The path to big tweet\n",
    "SUA_SHAPE = \"../data/SUA/SUA_2016_AUST.shp\"   ### SUA shape file\n",
    "CHUNK_SIZE = 1024 * 1024 * 600  # 600 MB\n",
    "# CHUNK_SIZE = 1024 * 1024 * 50  # 50 MB\n",
    "# START_CHUNK = 0   ## start chunk\n",
    "# END_CHUNK = 1   ## End chunk\n",
    "START_CHUNK = 101   ## start chunk\n",
    "END_CHUNK = 120   ## End chunk\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "# read sua_file\n",
    "gdf = gpd.read_file(SUA_SHAPE)\n",
    "sua_gdf = gdf\n",
    "sua_gdf['state'] = sua_gdf.apply(lambda x: STATE_CODE[int(x[\"SUA_CODE16\"][0])], axis = 1)\n",
    "sua_gdf[\"geo_back\"] = sua_gdf.geometry\n",
    "sua_gdf = sua_gdf.set_index(\"SUA_CODE16\")\n",
    "\n",
    "## prepare empty dataframe\n",
    "\n",
    "\n",
    "tweet_file = open(TWEET_PATH, 'rb')\n",
    "for i in range(START_CHUNK):\n",
    "    chunk = tweet_file.read(CHUNK_SIZE)\n",
    "\n",
    "city_list = []\n",
    "\n",
    "for chunk in range(START_CHUNK,END_CHUNK):\n",
    "    print(\"\\n\\n__________ Iteration {} __________\\nstart searching\".format(chunk))\n",
    "    \n",
    "    ## select chunk of data\n",
    "    chunk_f = tweet_file.read(CHUNK_SIZE)\n",
    "    if not chunk_f:\n",
    "        break\n",
    "    ## Save the chunk to a new file\n",
    "    input_file = CHUNK_PATH + 'chunk_' + str(chunk) + '.json'\n",
    "    with open(input_file, 'wb') as chunk_file:\n",
    "        chunk_file.write(chunk_f)\n",
    "    print(\"Chunk file saved\")\n",
    "    \n",
    "    columns = [\"id\", \"author_id\", \"content\", \"time\", \"tokens\", 'sentiment', \"area_name\", \"geometry\"]\n",
    "    tweet_gdf = gpd.GeoDataFrame(pd.DataFrame(columns=columns))\n",
    "    tweet_gdf = tweet_gdf.set_index(\"id\")\n",
    "    tweet_gdf = tweet_gdf.set_crs({'init': 'epsg:3857'})\n",
    "    \n",
    "    ## reading file\n",
    "    tweets_files =  open(input_file,'r',encoding='utf-8')\n",
    "    print(\"File readed\")\n",
    "    tweet_str= ''\n",
    "    tweets_files.readline()\n",
    "    # while True:\n",
    "    while True:\n",
    "        ## search tweet\n",
    "        new_line= tweets_files.readline()\n",
    "        if len(new_line)<6:\n",
    "            break\n",
    "        if new_line[:6]=='{\"id\":' and (new_line[-5:] == \"]}},\\n\" or new_line[-4:] == \"}]}}\\n\"):\n",
    "            if new_line[-5:] == \"]}},\\n\":\n",
    "                tweet_str = new_line[:-2]\n",
    "            else:\n",
    "                tweet_str = new_line[:-1]\n",
    "\n",
    "            new_line=tweets_files.readline()\n",
    "            tweet_json=json.loads(tweet_str)\n",
    "            \n",
    "            if \"includes\" in tweet_json[\"doc\"] and \"places\" in tweet_json[\"doc\"][\"includes\"]:\n",
    "                area_name = tweet_json[\"doc\"][\"includes\"][\"places\"][0][\"full_name\"].lower()\n",
    "                if area_name not in DROP_PROTO:\n",
    "                    location = tweet_json[\"doc\"][\"includes\"][\"places\"][0][\"geo\"][\"bbox\"]\n",
    "                    geometry = Polygon([(location[2], location[3]), (location[2], location[1]), (location[0], location[1]), (location[0], location[3])])\n",
    "                    data = [\n",
    "                        int(tweet_json[\"doc\"][\"data\"]['author_id']), \n",
    "                        tweet_json[\"doc\"][\"data\"]['text'], \n",
    "                        tweet_json[\"doc\"][\"data\"][\"created_at\"],\n",
    "                        tweet_json[\"value\"][\"tokens\"], tweet_json[\"doc\"][\"data\"]['sentiment'], \n",
    "                        tweet_json[\"doc\"][\"includes\"][\"places\"][0][\"full_name\"].lower(), \n",
    "                        geometry\n",
    "                    ]\n",
    "                    tweet_gdf.loc[int(tweet_json[\"id\"])] = data\n",
    "                    city_list.append(tweet_json[\"doc\"][\"includes\"][\"places\"][0][\"full_name\"].lower())\n",
    "        else:\n",
    "            break\n",
    "    print(\"End searching\")\n",
    "    if(tweet_gdf.empty):\n",
    "        print(\"Empty dataframe\")\n",
    "        continue\n",
    "    tweet_gdf[\"area_t\"] = tweet_gdf.area\n",
    "    tweet_gdf.index = tweet_gdf.index.astype(int)\n",
    "\n",
    "    sql_tweet_to_sua = \"SELECT id, SUA_NAME16 AS SUA_NAME, state, MAX(intersection) AS max_inter FROM match_gdf GROUP BY id\"\n",
    "    match_gdf = gpd.sjoin(tweet_gdf[[\"geometry\"]], sua_gdf[[\"state\", \"geometry\", \"SUA_NAME16\", \"geo_back\"]], how='inner', op='intersects')\n",
    "    match_gdf[\"intersection\"] = match_gdf.apply(lambda x: x[\"geometry\"].intersection(x[\"geo_back\"]).area, axis = 1)\n",
    "    match_gdf = match_gdf[[\"index_right\", \"state\", \"SUA_NAME16\", \"intersection\"]].reset_index()\n",
    "    match_gdf = ps.sqldf(sql_tweet_to_sua)\n",
    "    merge_gdf = pd.merge(tweet_gdf, match_gdf, on = \"id\", how = \"inner\").rename(columns={\"index_right\": \"SUA\"}).drop([\"max_inter\", \"area_t\", \"geometry\"], axis=1)\n",
    "    merge_gdf = pd.DataFrame(merge_gdf)\n",
    "\n",
    "    # output_file = OUT_DIR + \"filtered_{}.csv\".format(chunk)\n",
    "    # merge_gdf.to_csv(output_file)\n",
    "    \n",
    "    output_file = OUT_DIR + \"filtered_{}.json\".format(chunk)\n",
    "    merge_gdf.to_json(output_file, orient='records')\n",
    "    print(\"file saved: \", output_file)\n",
    "    tweets_files.close()\n",
    "    os.remove(input_file)\n",
    "    print(\"file deleted: \", output_file)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['filtered_56.json', 'filtered_10.json', 'filtered_43.json', 'filtered_6.json', 'filtered_48.json', 'filtered_51.json', 'filtered_12.json', 'filtered_49.json', 'filtered_71.json', 'filtered_11.json', 'filtered_5.json', 'filtered_62.json', 'filtered_35.json', 'filtered_99.json', 'filtered_41.json', 'filtered_30.json', 'filtered_52.json', 'filtered_93.json', 'filtered_68.json', 'filtered_75.json', 'filtered_40.json', 'filtered_16.json', 'filtered_4.json', 'filtered_61.json', 'filtered_53.json', 'filtered_50.json', 'filtered_64.json', 'filtered_13.json', 'filtered_63.json', 'filtered_45.json', 'filtered_39.json', 'filtered_94.json', 'filtered_38.json', 'filtered_17.json', 'filtered_79.json', 'filtered_84.json', 'filtered_21.json', 'filtered_96.json', 'filtered_70.json', 'filtered_0 copy.json', 'filtered_44.json', 'filtered_1.json', 'filtered_54.json', 'filtered_0.json', 'filtered_36.json', 'filtered_82.json', 'filtered_80.json', 'filtered_31.json', 'filtered_14.json', 'filtered_58.json', 'filtered_77.json', 'filtered_55.json', 'filtered_27.json', 'filtered_85.json', 'filtered_91.json', 'filtered_78.json', 'filtered_33.json', 'filtered_7.json', 'filtered_29.json', 'filtered_3.json', 'filtered_57.json', 'filtered_86.json', 'filtered_90.json', 'filtered_66.json', 'filtered_37.json', 'filtered_23.json', 'filtered_67.json', 'filtered_88.json', 'filtered_100.json', 'filtered_15.json', 'filtered_89.json', 'filtered_42.json', 'filtered_98.json', 'filtered_2.json', 'filtered_19.json', 'filtered_9.json', 'filtered_32.json', 'filtered_47.json', 'filtered_59.json', 'filtered_24.json', 'filtered_60.json', 'filtered_76.json', 'filtered_28.json', 'filtered_34.json', 'filtered_92.json', 'filtered_74.json', 'filtered_69.json', 'filtered_65.json', 'filtered_20.json', 'filtered_97.json', 'filtered_83.json', 'filtered_18.json', 'filtered_8.json', 'filtered_22.json', 'filtered_95.json', 'filtered_72.json', 'filtered_73.json', 'filtered_25.json', 'filtered_46.json', 'filtered_81.json', 'filtered_87.json']\n",
      "                   id            author_id  \\\n",
      "0        1.526937e+18           1656731628   \n",
      "1        1.527064e+18           2985147727   \n",
      "2        1.526906e+18            505736821   \n",
      "3        1.527019e+18  1219495123332976640   \n",
      "4        1.526925e+18  1515846669065396229   \n",
      "...               ...                  ...   \n",
      "1491433  1.548919e+18             46534278   \n",
      "1491434  1.548920e+18             93730228   \n",
      "1491435  1.548928e+18  1167693487870898177   \n",
      "1491436  1.548920e+18           1922975671   \n",
      "1491437  1.548920e+18           1922975671   \n",
      "\n",
      "                                                   content  \\\n",
      "0        @ChristyDanFan @WizePenguin Your friend is dum...   \n",
      "1                 @ChristyDanFan Is that one person geez .   \n",
      "2        Step closer to getting a diagnosis (doc found ...   \n",
      "3                             @TheAusMaverick @da_huw Hot!   \n",
      "4                                  @_viixw Ø§Ù„Ø­Ø±ÙŠØ© Ø§Ù‡Ù… Ø­Ø§Ø¬Ø©   \n",
      "...                                                    ...   \n",
      "1491433  3 dias p gloriosa senhora - mas nÃ£o tao senhor...   \n",
      "1491434           @JL_Whitaker Yes, Flop, Stormâ€™s daughter   \n",
      "1491435                                @LynneCampbell5 ðŸ’”ðŸ’šðŸ’š   \n",
      "1491436  Purtages na yan napa Melbourne ng wala sa oras...   \n",
      "1491437                                        Ang galengg   \n",
      "\n",
      "                             time  \\\n",
      "0        2022-05-18T14:44:12.000Z   \n",
      "1        2022-05-18T23:08:08.000Z   \n",
      "2        2022-05-18T12:41:23.000Z   \n",
      "3        2022-05-18T20:10:00.000Z   \n",
      "4        2022-05-18T13:56:45.000Z   \n",
      "...                           ...   \n",
      "1491433  2022-07-18T06:35:55.000Z   \n",
      "1491434  2022-07-18T06:38:06.000Z   \n",
      "1491435  2022-07-18T07:10:04.000Z   \n",
      "1491436  2022-07-18T06:36:37.000Z   \n",
      "1491437  2022-07-18T06:36:41.000Z   \n",
      "\n",
      "                                                    tokens  sentiment  \\\n",
      "0                   Your|friend|dumber|than|doggie|Doo|doo   0.100000   \n",
      "1                                     that|one|person|geez   0.000000   \n",
      "2        Step|closer|getting|diagnosis|doc|found|have|h...  -0.224490   \n",
      "3                                                      Hot   0.000000   \n",
      "4                                                            0.000000   \n",
      "...                                                    ...        ...   \n",
      "1491433  dias|gloriosa|senhora|mas|tao|senhora|assim|mi...   0.000000   \n",
      "1491434                            Yes|Flop|Storm|daughter  -0.166667   \n",
      "1491435                                                      0.000000   \n",
      "1491436  Purtages|yan|napa|Melbourne|wala|orass|hahahha...   0.000000   \n",
      "1491437                                        Ang|galengg   0.000000   \n",
      "\n",
      "                         area_name                                 SUA_NAME  \\\n",
      "0          sydney, new south wales                                   Sydney   \n",
      "1        adelaide, south australia                                 Adelaide   \n",
      "2              melbourne, victoria                                Melbourne   \n",
      "3              melbourne, victoria                                Melbourne   \n",
      "4         perth, western australia                                    Perth   \n",
      "...                            ...                                      ...   \n",
      "1491433    sydney, new south wales                                   Sydney   \n",
      "1491434    uralla, new south wales  Not in any Significant Urban Area (NSW)   \n",
      "1491435   perth, western australia                                    Perth   \n",
      "1491436  adelaide, south australia                                 Adelaide   \n",
      "1491437  adelaide, south australia                                 Adelaide   \n",
      "\n",
      "                     state  \n",
      "0          new south wales  \n",
      "1          south australia  \n",
      "2                 victoria  \n",
      "3                 victoria  \n",
      "4        western australia  \n",
      "...                    ...  \n",
      "1491433    new south wales  \n",
      "1491434    new south wales  \n",
      "1491435  western australia  \n",
      "1491436    south australia  \n",
      "1491437    south australia  \n",
      "\n",
      "[1491438 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "OUT_DIR = \"../data/filt_chunk/\"   ### filtered json file location\n",
    "LARGE_DF_DIR = \"../data/filt_tweet.json\"\n",
    "\n",
    "file_names = os.listdir(OUT_DIR)\n",
    "\n",
    "print(file_names)\n",
    "large_df = pd.read_json(OUT_DIR+file_names[0])\n",
    "\n",
    "for i in range(1, len(file_names)):\n",
    "    cur_df = pd.read_json(OUT_DIR+file_names[i])\n",
    "    large_df = large_df.append(cur_df)\n",
    "\n",
    "large_df = large_df.reset_index(drop=True)\n",
    "print(large_df)\n",
    "large_df.to_json(LARGE_DF_DIR, orient='records')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
